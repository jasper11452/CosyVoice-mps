#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CosyVoice MPS Web UI
é€‚ç”¨äº macOS Apple Silicon (M1/M2/M3/M4)
"""
import os
import sys

# ============== MPS (Apple Silicon) é€‚é… ==============
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['TORCHAUDIO_USE_BACKEND_DISPATCHER'] = '0'
# ======================================================

import argparse
import gradio as gr
import numpy as np
import torch
import soundfile as sf
import librosa
import random

# JIT ç¦ç”¨ï¼ˆMPS å…¼å®¹æ€§ï¼‰
if torch.backends.mps.is_available():
    torch.jit.script_method = lambda fn, _rcb=None: fn
    torch.jit.script = lambda obj, *args, **kwargs: obj
    print("âœ… MPS æ£€æµ‹åˆ°ï¼ŒJIT å·²ç¦ç”¨")

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(ROOT_DIR, 'third_party/Matcha-TTS'))

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.common import set_all_random_seed

# ============== é…ç½® ==============
MAX_VAL = 0.8
SAMPLE_RATE = 24000

# ============== å·¥å…·å‡½æ•° ==============

def load_wav(wav_path, target_sr=16000):
    """ä½¿ç”¨ soundfile åŠ è½½éŸ³é¢‘å¹¶é‡é‡‡æ ·"""
    speech, sr = sf.read(wav_path)
    if len(speech.shape) > 1:
        speech = speech[:, 0]  # å–ç¬¬ä¸€ä¸ªå£°é“
    if sr != target_sr:
        speech = librosa.resample(speech, orig_sr=sr, target_sr=target_sr)
    return torch.from_numpy(speech).float().unsqueeze(0)


def get_audio_info(wav_path):
    """è·å–éŸ³é¢‘ä¿¡æ¯"""
    info = sf.info(wav_path)
    return info.samplerate, info.duration


def postprocess(speech, top_db=60, hop_length=220, win_length=440):
    """åå¤„ç†éŸ³é¢‘"""
    speech_np = speech.cpu().numpy().squeeze()
    speech_np, _ = librosa.effects.trim(
        speech_np, top_db=top_db,
        frame_length=win_length,
        hop_length=hop_length
    )
    speech = torch.from_numpy(speech_np).unsqueeze(0)
    if speech.abs().max() > MAX_VAL:
        speech = speech / speech.abs().max() * MAX_VAL
    # æ·»åŠ å°¾éƒ¨é™éŸ³
    silence = torch.zeros(1, int(SAMPLE_RATE * 0.2))
    speech = torch.cat([speech, silence], dim=1)
    return speech


def generate_seed():
    """ç”Ÿæˆéšæœºç§å­"""
    return random.randint(1, 100000000)


# ============== ä¸»ç”Ÿæˆå‡½æ•° ==============

def generate_audio(
    tts_text,
    mode,
    sft_speaker,
    prompt_text,
    prompt_wav_upload,
    prompt_wav_record,
    instruct_text,
    seed,
    speed
):
    """ç”ŸæˆéŸ³é¢‘"""
    if not tts_text.strip():
        yield None, "è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬"
        return
    
    set_all_random_seed(seed)
    
    # ç¡®å®šå‚è€ƒéŸ³é¢‘
    prompt_wav = prompt_wav_upload if prompt_wav_upload else prompt_wav_record
    
    try:
        if mode == "é¢„è®­ç»ƒéŸ³è‰²":
            # SFT æ¨¡å¼
            if not sft_speaker:
                yield None, "è¯·é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²"
                return
            
            result = []
            for output in cosyvoice.inference_sft(tts_text, sft_speaker, stream=False):
                result.append(output['tts_speech'])
            
            if result:
                speech = torch.cat(result, dim=1)
                speech = postprocess(speech)
                audio_np = speech.cpu().numpy().squeeze()
                yield (SAMPLE_RATE, audio_np), "âœ… ç”ŸæˆæˆåŠŸ"
            else:
                yield None, "ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰è¾“å‡º"
                
        elif mode == "3sæé€Ÿå¤åˆ»":
            # Zero-shot æ¨¡å¼
            if not prompt_wav:
                yield None, "è¯·ä¸Šä¼ æˆ–å½•åˆ¶å‚è€ƒéŸ³é¢‘"
                return
            if not prompt_text.strip():
                yield None, "è¯·è¾“å…¥å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬"
                return
            
            # æ£€æŸ¥éŸ³é¢‘æ—¶é•¿
            sr, duration = get_audio_info(prompt_wav)
            if duration > 30:
                yield None, "å‚è€ƒéŸ³é¢‘ä¸èƒ½è¶…è¿‡30ç§’"
                return
            
            prompt_speech_16k = load_wav(prompt_wav, 16000)
            
            result = []
            for output in cosyvoice.inference_zero_shot(
                tts_text, prompt_text, prompt_speech_16k, stream=False
            ):
                result.append(output['tts_speech'])
            
            if result:
                speech = torch.cat(result, dim=1)
                speech = postprocess(speech)
                audio_np = speech.cpu().numpy().squeeze()
                yield (SAMPLE_RATE, audio_np), "âœ… ç”ŸæˆæˆåŠŸ"
            else:
                yield None, "ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰è¾“å‡º"
                
        elif mode == "è·¨è¯­ç§å¤åˆ»":
            # Cross-lingual æ¨¡å¼
            if not prompt_wav:
                yield None, "è¯·ä¸Šä¼ æˆ–å½•åˆ¶å‚è€ƒéŸ³é¢‘"
                return
            
            sr, duration = get_audio_info(prompt_wav)
            if duration > 30:
                yield None, "å‚è€ƒéŸ³é¢‘ä¸èƒ½è¶…è¿‡30ç§’"
                return
            
            prompt_speech_16k = load_wav(prompt_wav, 16000)
            
            result = []
            for output in cosyvoice.inference_cross_lingual(
                tts_text, prompt_speech_16k, stream=False
            ):
                result.append(output['tts_speech'])
            
            if result:
                speech = torch.cat(result, dim=1)
                speech = postprocess(speech)
                audio_np = speech.cpu().numpy().squeeze()
                yield (SAMPLE_RATE, audio_np), "âœ… ç”ŸæˆæˆåŠŸ"
            else:
                yield None, "ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰è¾“å‡º"
                
        elif mode == "è‡ªç„¶è¯­è¨€æ§åˆ¶":
            # Instruct æ¨¡å¼ (éœ€è¦ instruct æ¨¡å‹æˆ–ä½¿ç”¨ instruct2)
            if not prompt_wav:
                yield None, "è¯·ä¸Šä¼ æˆ–å½•åˆ¶å‚è€ƒéŸ³é¢‘"
                return
            if not instruct_text.strip():
                yield None, "è¯·è¾“å…¥æŒ‡ä»¤æ–‡æœ¬"
                return
            
            sr, duration = get_audio_info(prompt_wav)
            if duration > 30:
                yield None, "å‚è€ƒéŸ³é¢‘ä¸èƒ½è¶…è¿‡30ç§’"
                return
            
            prompt_speech_16k = load_wav(prompt_wav, 16000)
            
            result = []
            for output in cosyvoice.inference_instruct2(
                tts_text, instruct_text, prompt_speech_16k, stream=False
            ):
                result.append(output['tts_speech'])
            
            if result:
                speech = torch.cat(result, dim=1)
                speech = postprocess(speech)
                audio_np = speech.cpu().numpy().squeeze()
                yield (SAMPLE_RATE, audio_np), "âœ… ç”ŸæˆæˆåŠŸ"
            else:
                yield None, "ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰è¾“å‡º"
        else:
            yield None, f"æœªçŸ¥æ¨¡å¼: {mode}"
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        yield None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def update_ui(mode):
    """æ ¹æ®æ¨¡å¼æ›´æ–° UI å¯è§æ€§"""
    if mode == "é¢„è®­ç»ƒéŸ³è‰²":
        return (
            gr.update(visible=True),   # sft_speaker
            gr.update(visible=False),  # prompt_text
            gr.update(visible=False),  # prompt_wav_upload
            gr.update(visible=False),  # prompt_wav_record
            gr.update(visible=False),  # instruct_text
        )
    elif mode == "3sæé€Ÿå¤åˆ»":
        return (
            gr.update(visible=False),
            gr.update(visible=True),
            gr.update(visible=True),
            gr.update(visible=True),
            gr.update(visible=False),
        )
    elif mode == "è·¨è¯­ç§å¤åˆ»":
        return (
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=True),
            gr.update(visible=True),
            gr.update(visible=False),
        )
    elif mode == "è‡ªç„¶è¯­è¨€æ§åˆ¶":
        return (
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=True),
            gr.update(visible=True),
            gr.update(visible=True),
        )


# ============== ä¸»ç¨‹åº ==============

def main():
    global cosyvoice, SAMPLE_RATE
    
    parser = argparse.ArgumentParser(description='CosyVoice MPS Web UI')
    parser.add_argument('--port', type=int, default=50000, help='æœåŠ¡ç«¯å£')
    parser.add_argument('--model_dir', type=str, default='pretrained_models/CosyVoice2-0.5B',
                        help='æ¨¡å‹ç›®å½•')
    parser.add_argument('--share', action='store_true', help='åˆ›å»ºå…¬å¼€é“¾æ¥')
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_dir}")
    print(f"è®¾å¤‡: {'MPS' if torch.backends.mps.is_available() else 'CPU'}")
    
    cosyvoice = CosyVoice2(args.model_dir)
    SAMPLE_RATE = cosyvoice.sample_rate
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {cosyvoice.model.device}")
    
    # è·å–å¯ç”¨éŸ³è‰²
    available_spks = cosyvoice.list_available_spks()
    print(f"å¯ç”¨éŸ³è‰²: {available_spks}")
    
    # åˆ›å»º Gradio ç•Œé¢
    with gr.Blocks(title="CosyVoice MPS", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¤ CosyVoice MPS
        **åœ¨ Apple Silicon ä¸Šè¿è¡Œçš„è¯­éŸ³åˆæˆæ¨¡å‹**
        
        æ”¯æŒæ¨¡å¼ï¼šé¢„è®­ç»ƒéŸ³è‰²ã€3sæé€Ÿå¤åˆ»ã€è·¨è¯­ç§å¤åˆ»ã€è‡ªç„¶è¯­è¨€æ§åˆ¶
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ¨¡å¼é€‰æ‹©
                mode = gr.Radio(
                    choices=["é¢„è®­ç»ƒéŸ³è‰²", "3sæé€Ÿå¤åˆ»", "è·¨è¯­ç§å¤åˆ»", "è‡ªç„¶è¯­è¨€æ§åˆ¶"],
                    value="3sæé€Ÿå¤åˆ»",
                    label="åˆæˆæ¨¡å¼"
                )
                
                # é¢„è®­ç»ƒéŸ³è‰²é€‰æ‹©
                sft_speaker = gr.Dropdown(
                    choices=available_spks,
                    value=available_spks[0] if available_spks else None,
                    label="é¢„è®­ç»ƒéŸ³è‰²",
                    visible=False
                )
                
                # åˆæˆæ–‡æœ¬
                tts_text = gr.Textbox(
                    label="åˆæˆæ–‡æœ¬",
                    placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                    lines=3
                )
                
                # å‚è€ƒéŸ³é¢‘æ–‡æœ¬
                prompt_text = gr.Textbox(
                    label="å‚è€ƒéŸ³é¢‘æ–‡æœ¬",
                    placeholder="è¯·è¾“å…¥å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬å†…å®¹...",
                    lines=2,
                    visible=True
                )
                
                # å‚è€ƒéŸ³é¢‘ä¸Šä¼ 
                prompt_wav_upload = gr.Audio(
                    label="ä¸Šä¼ å‚è€ƒéŸ³é¢‘",
                    type="filepath",
                    visible=True
                )
                
                # å‚è€ƒéŸ³é¢‘å½•åˆ¶
                prompt_wav_record = gr.Audio(
                    label="å½•åˆ¶å‚è€ƒéŸ³é¢‘",
                    sources=["microphone"],
                    type="filepath",
                    visible=True
                )
                
                # æŒ‡ä»¤æ–‡æœ¬
                instruct_text = gr.Textbox(
                    label="æŒ‡ä»¤æ–‡æœ¬",
                    placeholder="ä¾‹å¦‚ï¼šç”¨å››å·è¯è¯´è¿™å¥è¯",
                    visible=False
                )
                
                with gr.Row():
                    seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                    seed_btn = gr.Button("ğŸ² éšæœº", size="sm")
                
                speed = gr.Slider(
                    minimum=0.5, maximum=2.0, value=1.0, step=0.1,
                    label="è¯­é€Ÿ"
                )
                
                generate_btn = gr.Button("ğŸµ ç”ŸæˆéŸ³é¢‘", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                output_audio = gr.Audio(label="ç”Ÿæˆçš„éŸ³é¢‘", type="numpy")
                output_text = gr.Textbox(label="çŠ¶æ€", interactive=False)
        
        # äº‹ä»¶ç»‘å®š
        mode.change(
            fn=update_ui,
            inputs=[mode],
            outputs=[sft_speaker, prompt_text, prompt_wav_upload, prompt_wav_record, instruct_text]
        )
        
        seed_btn.click(
            fn=generate_seed,
            outputs=[seed]
        )
        
        generate_btn.click(
            fn=generate_audio,
            inputs=[
                tts_text, mode, sft_speaker, prompt_text,
                prompt_wav_upload, prompt_wav_record, instruct_text,
                seed, speed
            ],
            outputs=[output_audio, output_text]
        )
    
    # å¯åŠ¨æœåŠ¡
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share
    )


if __name__ == "__main__":
    main()
